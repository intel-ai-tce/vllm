# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  vllm-service-cpu:
    image: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.10.0
    container_name: vllm-service-cpu
    ports:
      - "8001:80"
    volumes:
      - "${MODEL_CACHE:-./data}:/root/.cache/huggingface/hub"
    shm_size: 128g
    #cpuset: "0-31"
    #privileged: true
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_CPU_SGL_KERNEL: 1
      VLLM_CPU_KVCACHE_SPACE: 40
    command: --model meta-llama/Llama-3.1-8B --host 0.0.0.0 --port 80 --dtype bfloat16 --distributed-executor-backend mp --block-size 128 --enforce-eager --tensor-parallel-size 1
  vllm-service-gaudi:
    # image: vault.habana.ai/gaudi-docker/1.21.3/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest
    image: ${REGISTRY:-opea}/vllm-gaudi:${TAG:-1.3.1}
    container_name: vllm-service-gaudi
    ports:
      - "8002:80"
    volumes:
      - "${MODEL_CACHE:-./data}:/root/.cache/huggingface/hub"
      #shm_size: 128g
    #cpuset: "160-191"
    #privileged: true
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      HABANA_VISIBLE_DEVICES: all
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      NUM_CARDS: ${NUM_CARDS}
      VLLM_TORCH_PROFILER_DIR: "/mnt"
      PYTHONWARNINGS: "ignore"
    runtime: habana
    cap_add:
      - SYS_NICE
    ipc: host
    command: --model meta-llama/Llama-3.1-8B --tensor-parallel-size $NUM_CARDS --host 0.0.0.0 --port 80 --block-size 128 --max-num-seqs 256 --max-seq-len-to-capture 2048      
    #command: --model $LLM_MODEL_ID --tensor-parallel-size $NUM_CARDS --host 0.0.0.0 --port 80 --block-size 128 --max-num-seqs 256 --max-seq-len-to-capture 2048      
  vllm-ci-test:
    image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f-cpu
    container_name: vllm-ci-test
    volumes:
      - "${MODEL_CACHE:-./data}:/root/.cache/huggingface/hub"
      - "../benchmarks:/workspace/benchmarks"
      - "../.buildkite:/workspace/.buildkite"
    shm_size: 128g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      VLLM_CPU_KVCACHE_SPACE: 40
      ON_CPU: 1
      REMOTE_HOST: nginx-lb
      REMOTE_PORT: 80
    #command: --model $LLM_MODEL_ID --host 0.0.0.0 --port 80 --dtype bfloat16 --distributed-executor-backend mp --block-size 128 --enforce-eager --tensor-parallel-size 1
    entrypoint: tail -f /dev/null
    #command: /bin/bash
  nginx-lb-server:
    image: nginx-lb:${TAG:-latest}
    container_name: nginx-lb
    ports:
      - "${NGINX_PORT:-80}:80"
    volumes:
      - "./nginx_conf/:/etc/nginx/conf.d/"
      - "./nginx_conf/nginx.conf:/etc/nginx/nginx.conf:ro"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
    ipc: host
    restart: always

networks:
  default:
    driver: bridge
